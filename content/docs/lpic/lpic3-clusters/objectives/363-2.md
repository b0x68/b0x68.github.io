---
title: "363.2 Ceph Storage Clusters (weight: 8)"
#bookCollapseSection: true
---

# Tech Tutorial: 363.2 Ceph Storage Clusters (weight: 8)

## Introduction

Ceph is a highly resilient and scalable storage solution that allows enterprises to build and manage vast amounts of data in a distributed storage architecture. In this tutorial, we will explore how to set up and manage a basic Ceph storage cluster. We will cover some essential commands and configurations required for day-to-day management and troubleshooting of Ceph clusters.

## Exam Objective:

- Understand the architecture and deployment of Ceph storage clusters.
- Be able to perform basic Ceph cluster operations and maintenance.

## Key Knowledge Areas:

- Ceph cluster architecture
- Basic Ceph commands and usage
- Monitoring and maintaining a Ceph cluster

## Utilities:

- ceph
- rados
- rbd
- ceph-mon
- ceph-osd

## Step-by-Step Guide

### Step 1: Understanding Ceph Components

Ceph primarily consists of the following components:

- **Ceph OSDs (Object Storage Daemons)**: Stores data as objects on storage nodes.
- **Ceph Monitor (ceph-mon)**: Keeps a master copy of the cluster map.
- **Ceph Manager (ceph-mgr)**: Provides additional monitoring and management.
- **Ceph RADOS Block Device (RBD)**: Allows block-level storage access.

### Step 2: Setting Up a Ceph Cluster

Before beginning, ensure that you have at least three nodes to set up a minimal cluster.

1. **Install Ceph** on all nodes:

```bash
sudo apt update
sudo apt install ceph-deploy
```

2. **Create a new cluster directory** and cluster configuration:

```bash
mkdir my-ceph-cluster
cd my-ceph-cluster
ceph-deploy new node1 node2 node3
```

3. **Install Ceph on all nodes**:

```bash
ceph-deploy install node1 node2 node3
```

4. **Deploy the initial monitor(s) and gather the keys**:

```bash
ceph-deploy mon create-initial
```

### Step 3: Adding OSDs

OSDs are the heart of the Ceph storage, handling the storage of data in terms of objects.

```bash
ceph-deploy osd create --data /dev/sdb node1
```

Replace `/dev/sdb` with your storage drive identifier.

### Step 4: Basic Ceph Commands

- **Check cluster health**:

```bash
ceph health
```

- **List all OSDs**:

```bash
ceph osd ls
```

- **Get cluster status**:

```bash
ceph -s
```

### Step 5: Using RADOS and RBD Utilities

- **Create a pool with RADOS**:

```bash
ceph osd pool create mypool 128 128
```

- **List all pools**:

```bash
rados lspools
```

- **Create an RBD (block device)**:

```bash
rbd create mypool/myvolume --size 1024
```

- **Map the RBD block device**:

```bash
rbd map mypool/myvolume
```

### Step 6: Monitoring and Maintaining

- **Monitor cluster utilization**:

```bash
ceph df
```

- **Check OSD status**:

```bash
ceph osd stat
```

- **Repairing OSDs**:

```bash
ceph osd repair osd.<osd-id>
```

Replace `<osd-id>` with the actual OSD ID.

## Conclusion

In this tutorial, we've gone through the basics of setting up and managing a Ceph storage cluster. The commands and procedures outlined provide a foundation for further exploration and management of more complex Ceph functionalities. Ceph's scalable and resilient nature makes it an ideal solution for large-scale storage requirements in modern data centers.